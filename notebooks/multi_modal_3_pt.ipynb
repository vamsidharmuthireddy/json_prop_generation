{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(1000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 1 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\n",
    "from transformers import VisionEncoderDecoderModel , ViTFeatureExtractor\n",
    "from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "%autosave 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.kernel.execute('notebook_name = \"' + IPython.notebook.notebook_name + '\"')\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('notebook_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# var kernel = IPython.notebook.kernel;\n",
    "# var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "# var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
    "# kernel.execute(command);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_modal.ipynb\n"
     ]
    }
   ],
   "source": [
    "if notebook_name is None:\n",
    "    notebook_name = 'multi_modal.ipynb'\n",
    "print(notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02-16-15-50\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "session_key = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(session_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvamsidhar_muthireddy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vamsidhar/Documents/purecode/solution_1/model_2/wandb/run-20230402_214552-1kbosh6m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vamsidhar_muthireddy/component-properties-generation/runs/1kbosh6m' target=\"_blank\">lilac-feather-16</a></strong> to <a href='https://wandb.ai/vamsidhar_muthireddy/component-properties-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vamsidhar_muthireddy/component-properties-generation' target=\"_blank\">https://wandb.ai/vamsidhar_muthireddy/component-properties-generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vamsidhar_muthireddy/component-properties-generation/runs/1kbosh6m' target=\"_blank\">https://wandb.ai/vamsidhar_muthireddy/component-properties-generation/runs/1kbosh6m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = notebook_name\n",
    "\n",
    "wandb.init(project=\"component-properties-generation\",\n",
    "        config={\n",
    "            \"learning_rate\": None,\n",
    "            \"epochs\":  None,\n",
    "            \"batch_size\":  None,\n",
    "            \"val_split\":  None,\n",
    "            \"dropout\":  None,\n",
    "            \"image_width\":  None,\n",
    "            \"image_height\":  None,\n",
    "            \"random_seed\":  None,\n",
    "            \"session_key\":  None,\n",
    "            \"model_name\": \"ResNet50V2\",\n",
    "            \"train_accuracy\": None,\n",
    "            \"val_accuracy\": None,\n",
    "            \"test_accuracy\": None,\n",
    "            \"class_weight_strategy\": None,\n",
    "            \"fine_tuning\": None,\n",
    "            \"pad_input\": None,\n",
    "            \"pad_strategy\":None\n",
    "        })\n",
    "\n",
    "w_config = wandb.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "w_config.learning_rate = 0.001\n",
    "w_config.epochs = 2\n",
    "w_config.batch_size = 32\n",
    "w_config.val_split = 0.2\n",
    "w_config.dropout = 0.1\n",
    "w_config.image_width = 128\n",
    "w_config.image_height = 128\n",
    "w_config.random_seed = 42\n",
    "w_config.session_key = session_key\n",
    "w_config.model_name = \"vit-gpt2\"\n",
    "w_config.train_accuracy = 0\n",
    "w_config.val_accuracy = 0\n",
    "w_config.test_accuracy = 0\n",
    "w_config.class_weight_strategy = \"Augmented\"    #keys: None, \"Original\", \"Augmented\"\n",
    "w_config.fine_tuning = False                     #Fine-tunes the model if True\n",
    "w_config.pad_input = True\n",
    "w_config.pad_strategy = 'pad_only'      #keys: 'pad_only', 'scale_and_pad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Organizing data for model training and inferencing\n",
    "\n",
    "\n",
    "\n",
    "Model training will happen on images from train_folder <br>\n",
    "train_folder = Training set (Will be split into train and validation to be used for model training)\n",
    "\n",
    "Model inferencing will happen on images from test_folder <br>\n",
    "test_folder = Test set (Will be used to evaluate the trained model for <b>inference</b>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vamsidhar/Documents/purecode/solution_1/test_2023-04-02-16-15-50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shuffle_dataset = True\n",
    "\n",
    "\n",
    "\n",
    "save_folder = '/home/vamsidhar/Documents/purecode/solution_1/test_' + str(w_config.session_key)\n",
    "print(save_folder)\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(save_folder, \"model_weights.h5\")\n",
    "architecture_save_path = os.path.join(save_folder, \"model_architecture.txt\")\n",
    "\n",
    "\n",
    "np.random.seed(w_config.random_seed)\n",
    "random.seed(w_config.random_seed)\n",
    "\n",
    "\n",
    "\n",
    "train_df_path = '/home/vamsidhar/Documents/purecode/solution_1/df_train.pkl'\n",
    "test_df_path = '/home/vamsidhar/Documents/purecode/solution_1/df_test.pkl'\n",
    "\n",
    "train_folder = '/home/vamsidhar/Documents/purecode/data_3/train/images'\n",
    "test_folder = '/home/vamsidhar/Documents/purecode/data_3/test/images'\n",
    "\n",
    "tokenizer_path = '/home/vamsidhar/Documents/purecode/solution_1/tokenizer.pkl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_train = joblib.load(train_df_path)\n",
    "df_test = joblib.load(test_df_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_weights(df):\n",
    "    cls = df.comp_label.unique()\n",
    "    \n",
    "    cls_count = df.comp_label.value_counts()\n",
    "    maximum = max(cls_count)\n",
    "    cls_count = maximum/cls_count\n",
    "    \n",
    "    weights = cls_count.to_dict()\n",
    "\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Switch' 'Checkbox' 'Card' 'Icon' 'Button']\n",
      "{3: 1.0, 4: 1.758058399696625, 2: 2.136405529953917, 1: 3.6049766718506997, 0: 14.578616352201259}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes_train = df_train.comp_type.unique()\n",
    "classes_test  = df_test.comp_type.unique()\n",
    "print(classes_train)\n",
    "\n",
    "num_classes = len(classes_train)\n",
    "\n",
    "\n",
    "class_weights = get_weights(df=df_train)\n",
    "    \n",
    "print(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "\n",
    "def generate_filenames_label(df, shuffle=True, split=False, split_ratio=w_config.val_split):\n",
    "    if not split:\n",
    "        file_name_img = df.path_images.values\n",
    "        properties = list(df.text.values)\n",
    "#         label = label_encoding(df)\n",
    "        label = df.comp_label.values\n",
    "    \n",
    "        return label, properties, file_name_img\n",
    "    \n",
    "    else:\n",
    "        df_temp = df.copy()\n",
    "        \n",
    "        df_val = df_temp.sample(frac = 0.2, random_state=42)\n",
    "        df_train = df_temp.drop(df_val.index)\n",
    "\n",
    "        df_val.reset_index(inplace=True)\n",
    "        df_train.reset_index(inplace=True)\n",
    "\n",
    "        file_name_img_train = df_train.path_images.values\n",
    "        properties_train = list(df_train.text.values)\n",
    "#         label_train = label_encoding(df_train)\n",
    "        label_train = df_train.comp_label.values\n",
    "        \n",
    "        \n",
    "        file_name_img_val = df_val.path_images.values\n",
    "        properties_val = list(df_val.text.values)\n",
    "#         label_val = label_encoding(df_val)\n",
    "        label_val = df_val.comp_label.values\n",
    "        \n",
    "        \n",
    "        return label_train, properties_train, file_name_img_train, label_val, properties_val, file_name_img_val  \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train, prop_train, file_name_train, label_val, prop_val, file_name_val = generate_filenames_label(df_train,\n",
    "                                                                                  shuffle=True,\n",
    "                                                                                  split=True,\n",
    "                                                                                  split_ratio=w_config.val_split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_test, prop_test, file_name_test  = generate_filenames_label(df_test, shuffle=False, split=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\n",
    "from transformers import TFVisionEncoderDecoderModel , ViTFeatureExtractor\n",
    "from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config : \n",
    "    ENCODER = \"google/vit-base-patch16-224\"\n",
    "    DECODER = \"gpt2\"\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VAL_BATCH_SIZE = 8\n",
    "    VAL_EPOCHS = 1\n",
    "    LR = 5e-5\n",
    "    SEED = 42\n",
    "    MAX_LEN = 350\n",
    "    SUMMARY_LEN = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    TRAIN_PCT = 0.95\n",
    "    NUM_WORKERS = mp.cpu_count()\n",
    "    EPOCHS = 3\n",
    "    IMG_SIZE = (224,224)\n",
    "    LABEL_MASK = -100\n",
    "    TOP_K = 1000\n",
    "    TOP_P = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "AutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412307/3810674018.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = datasets.load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsidhar/miniconda3/envs/purecode_cpu/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(config.ENCODER)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.DECODER)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# tokenizer = joblib.load(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# img = cv2.imread(df_train.path_images[0])/255\n",
    "\n",
    "# feature_extractor([img, img, img], return_tensors='tf').pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, props, tokenizer,\n",
    "                 feature_extractor, shuffle=True):\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "        self.props = props\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.df = df\n",
    "#         self.transform = transform\n",
    "#         self.root_dir = root_dir\n",
    "        self.max_length = 350\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.props[idx]\n",
    "        img_path = self.filenames[idx]\n",
    "#         img = self.__load_image__(img_path)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             img= self.transform(img)\n",
    "        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n",
    "        captions = self.tokenizer(caption,\n",
    "                                 padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                 max_length=self.max_length).input_ids\n",
    "        \n",
    "        captions = [caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(captions)}\n",
    "        return encoding\n",
    "    \n",
    "    \n",
    "#     def __load_image__(self, image_path):\n",
    "#         img = cv2.imread(image_path)\n",
    "#         img = self.__resize_image__(img, pad=True)\n",
    "            \n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        \n",
    "#         return img\n",
    "\n",
    "#     def __resize_image__(self, img, pad=True):\n",
    "#         old_size = img.shape[:2]\n",
    "#         ratio = float(self.dim[0])/max(old_size)\n",
    "#         new_size = tuple([int(x*ratio) for x in old_size])\n",
    "#         img_new = cv2.resize(img, (new_size[1], new_size[0])) \n",
    "\n",
    "#         #resize by padding\n",
    "#         delta_w = self.dim[1] - new_size[1]\n",
    "#         delta_h = self.dim[0] - new_size[0]\n",
    "#         top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "#         left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "#         color = [0, 0, 0]\n",
    "#         img_new = cv2.copyMakeBorder(img_new, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "#             value=color)\n",
    "\n",
    "        \n",
    "#         return img_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImgDataset(filenames=file_name_train, labels=label_train, props=prop_train,\n",
    "                           tokenizer=tokenizer,feature_extractor = feature_extractor )\n",
    "\n",
    "val_dataset = ImgDataset(filenames=file_name_val, labels=label_val, props=prop_val,\n",
    "                           tokenizer=tokenizer,feature_extractor = feature_extractor )\n",
    "\n",
    "test_dataset = ImgDataset(filenames=file_name_test, labels=label_val, props=prop_test,\n",
    "                           tokenizer=tokenizer,feature_extractor = feature_extractor )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.1.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.2.crossattention.masked_bias', 'h.0.crossattention.masked_bias', 'h.10.crossattention.masked_bias', 'h.5.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.0.ln_cross_attn.weight', 'h.5.crossattention.masked_bias', 'h.4.crossattention.c_proj.weight', 'h.9.crossattention.bias', 'h.10.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.2.crossattention.c_attn.weight', 'h.1.crossattention.bias', 'h.9.crossattention.c_attn.weight', 'h.11.crossattention.bias', 'h.10.crossattention.bias', 'h.9.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.bias', 'h.2.crossattention.bias', 'h.10.crossattention.q_attn.weight', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.8.crossattention.masked_bias', 'h.1.crossattention.c_proj.weight', 'h.7.ln_cross_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.masked_bias', 'h.1.ln_cross_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.weight', 'h.11.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.bias', 'h.2.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.0.crossattention.bias', 'h.8.crossattention.q_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.1.crossattention.masked_bias', 'h.5.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.bias', 'h.3.crossattention.bias', 'h.6.crossattention.bias', 'h.5.ln_cross_attn.weight', 'h.8.crossattention.bias', 'h.4.crossattention.bias', 'h.11.ln_cross_attn.weight', 'h.4.ln_cross_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.3.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.5.crossattention.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.masked_bias', 'h.4.crossattention.masked_bias', 'h.10.ln_cross_attn.weight', 'h.9.crossattention.masked_bias', 'h.0.crossattention.q_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(config.ENCODER, \n",
    "                                                                  config.DECODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.max_length = 350\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='VIT_large_gpt2',\n",
    "    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.VAL_BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1024,  \n",
    "    save_steps=2048, \n",
    "    warmup_steps=1024,  \n",
    "    learning_rate = 5e-5,\n",
    "    #max_steps=1500, # delete for full training\n",
    "    num_train_epochs = config.EPOCHS, #TRAIN_EPOCHS\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "import cv2\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=feature_extractor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='581' max='3315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 581/3315 1:57:43 < 9:15:51, 0.08 it/s, Epoch 0.52/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f105423b4adf4cc634b98cf4481e00717d8221271c0e5b06e1f86191b8e1036b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
