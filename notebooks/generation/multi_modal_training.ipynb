{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Quadro P5000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 1 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import io, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\n",
    "from transformers import VisionEncoderDecoderModel , ViTFeatureExtractor\n",
    "from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "%autosave 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers matplotlib joblib opencv-python scikit-learn datasets rouge-score\n",
    "# !pip install joblib\n",
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gdown\n",
    "\n",
    "# # url = \"https://drive.google.com/u/0/uc?id=1vrXaLX7k9jyLXeSCinxA6msUyKDrQTvz&export=download\"\n",
    "# url = \"https://drive.google.com/u/0/uc?id=1U-tbS_gRI3dKTBQ0OW6-xPLnJRGoOXFm&export=download\"\n",
    "# output = \"data_3.zip\"\n",
    "# gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !unzip ./data_3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>path_properties</th>\n",
       "      <th>path_images</th>\n",
       "      <th>comp_type</th>\n",
       "      <th>comp_label</th>\n",
       "      <th>name</th>\n",
       "      <th>token_single</th>\n",
       "      <th>token_dataset</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>Switch</td>\n",
       "      <td>0</td>\n",
       "      <td>MfaISCBZ87B55HsM6Mc6i</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"componentType\": \"Switch\", \"componentProps/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>Switch</td>\n",
       "      <td>0</td>\n",
       "      <td>TU60pICpP--GkI5DFnfSy</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"componentType\": \"Switch\", \"componentProps/di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>Switch</td>\n",
       "      <td>0</td>\n",
       "      <td>uqMDZbdXuZrM6Xfiie1ph</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"componentType\": \"Switch\", \"componentProps/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>Switch</td>\n",
       "      <td>0</td>\n",
       "      <td>oCn9YXPyrIyQdaujio-Pe</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"componentType\": \"Switch\", \"componentProps/de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>/home/vamsidhar/Documents/purecode/data_3/trai...</td>\n",
       "      <td>Switch</td>\n",
       "      <td>0</td>\n",
       "      <td>S3OgTsHHCPlhZn9L49OLt</td>\n",
       "      <td>[input_ids, attention_mask]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{\"componentType\": \"Switch\", \"componentProps/ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                    path_properties  \\\n",
       "0      0  /home/vamsidhar/Documents/purecode/data_3/trai...   \n",
       "1      1  /home/vamsidhar/Documents/purecode/data_3/trai...   \n",
       "2      2  /home/vamsidhar/Documents/purecode/data_3/trai...   \n",
       "3      3  /home/vamsidhar/Documents/purecode/data_3/trai...   \n",
       "4      4  /home/vamsidhar/Documents/purecode/data_3/trai...   \n",
       "\n",
       "                                         path_images comp_type  comp_label  \\\n",
       "0  /home/vamsidhar/Documents/purecode/data_3/trai...    Switch           0   \n",
       "1  /home/vamsidhar/Documents/purecode/data_3/trai...    Switch           0   \n",
       "2  /home/vamsidhar/Documents/purecode/data_3/trai...    Switch           0   \n",
       "3  /home/vamsidhar/Documents/purecode/data_3/trai...    Switch           0   \n",
       "4  /home/vamsidhar/Documents/purecode/data_3/trai...    Switch           0   \n",
       "\n",
       "                    name                 token_single token_dataset  \\\n",
       "0  MfaISCBZ87B55HsM6Mc6i  [input_ids, attention_mask]           NaN   \n",
       "1  TU60pICpP--GkI5DFnfSy  [input_ids, attention_mask]           NaN   \n",
       "2  uqMDZbdXuZrM6Xfiie1ph  [input_ids, attention_mask]           NaN   \n",
       "3  oCn9YXPyrIyQdaujio-Pe  [input_ids, attention_mask]           NaN   \n",
       "4  S3OgTsHHCPlhZn9L49OLt  [input_ids, attention_mask]           NaN   \n",
       "\n",
       "                                                text  \n",
       "0  {\"componentType\": \"Switch\", \"componentProps/de...  \n",
       "1  {\"componentType\": \"Switch\", \"componentProps/di...  \n",
       "2  {\"componentType\": \"Switch\", \"componentProps/de...  \n",
       "3  {\"componentType\": \"Switch\", \"componentProps/de...  \n",
       "4  {\"componentType\": \"Switch\", \"componentProps/ch...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "df_train = joblib.load('df_train.pkl')\n",
    "df_test = joblib.load('df_test.pkl')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data_3/train/images/Switch/MfaISCBZ87B55HsM6Mc6i.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['path_properties'] = df_train.path_properties.apply(lambda x: x.replace('/home/vamsidhar/Documents/purecode/', './'))\n",
    "df_train['path_images'] = df_train.path_images.apply(lambda x: x.replace('/home/vamsidhar/Documents/purecode/', './'))\n",
    "df_train.path_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data_3/test/images/Card/hBCqHmQDKXBB7f0tka2Mg.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['path_properties'] = df_test.path_properties.apply(lambda x: x.replace('/home/vamsidhar/Documents/purecode/', './'))\n",
    "df_test['path_images'] = df_test.path_images.apply(lambda x: x.replace('/home/vamsidhar/Documents/purecode/', './'))\n",
    "df_test.path_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notebook_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.kernel.execute('notebook_name = \"' + IPython.notebook.notebook_name + '\"')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('notebook_name = \"' + IPython.notebook.notebook_name + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%javascript\n",
    "# var kernel = IPython.notebook.kernel;\n",
    "# var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "# var command = \"notebook_name = \" + \"'\"+thename+\"'\";\n",
    "# kernel.execute(command);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi_modal.ipynb\n"
     ]
    }
   ],
   "source": [
    "if notebook_name is None:\n",
    "    notebook_name = 'multi_modal.ipynb'\n",
    "print(notebook_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-02-17-55-57\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "session_key = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(session_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# os.environ['WANDB_NOTEBOOK_NAME'] = notebook_name\n",
    "\n",
    "# wandb.init(project=\"component-properties-generation\",\n",
    "#         config={\n",
    "#             \"learning_rate\": None,\n",
    "#             \"epochs\":  None,\n",
    "#             \"batch_size\":  None,\n",
    "#             \"val_split\":  None,\n",
    "#             \"dropout\":  None,\n",
    "#             \"image_width\":  None,\n",
    "#             \"image_height\":  None,\n",
    "#             \"random_seed\":  None,\n",
    "#             \"session_key\":  None,\n",
    "#             \"model_name\": \"ResNet50V2\",\n",
    "#             \"train_accuracy\": None,\n",
    "#             \"val_accuracy\": None,\n",
    "#             \"test_accuracy\": None,\n",
    "#             \"class_weight_strategy\": None,\n",
    "#             \"fine_tuning\": None,\n",
    "#             \"pad_input\": None,\n",
    "#             \"pad_strategy\":None\n",
    "#         })\n",
    "\n",
    "# w_config = wandb.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# w_config.learning_rate = 0.001\n",
    "# w_config.epochs = 2\n",
    "# w_config.batch_size = 32\n",
    "# w_config.val_split = 0.2\n",
    "# w_config.dropout = 0.1\n",
    "# w_config.image_width = 128\n",
    "# w_config.image_height = 128\n",
    "# w_config.random_seed = 42\n",
    "# w_config.session_key = session_key\n",
    "# w_config.model_name = \"resnet50v2_gpt2\"\n",
    "# w_config.train_accuracy = 0\n",
    "# w_config.val_accuracy = 0\n",
    "# w_config.test_accuracy = 0\n",
    "# w_config.class_weight_strategy = \"Augmented\"    #keys: None, \"Original\", \"Augmented\"\n",
    "# w_config.fine_tuning = False                     #Fine-tunes the model if True\n",
    "# w_config.pad_input = True\n",
    "# w_config.pad_strategy = 'pad_only'      #keys: 'pad_only', 'scale_and_pad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "Organizing data for model training and inferencing\n",
    "\n",
    "\n",
    "\n",
    "Model training will happen on images from train_folder <br>\n",
    "train_folder = Training set (Will be split into train and validation to be used for model training)\n",
    "\n",
    "Model inferencing will happen on images from test_folder <br>\n",
    "test_folder = Test set (Will be used to evaluate the trained model for <b>inference</b>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vamsidhar/Documents/purecode/solution_1/test_2023-04-02-17-55-57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "\n",
    "save_folder = '/home/vamsidhar/Documents/purecode/solution_1/test_' + str(session_key)\n",
    "print(save_folder)\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "model_save_path = os.path.join(save_folder, \"model_weights.h5\")\n",
    "architecture_save_path = os.path.join(save_folder, \"model_architecture.txt\")\n",
    "\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "train_df_path = '/home/vamsidhar/Documents/purecode/solution_1/df_train.pkl'\n",
    "test_df_path = '/home/vamsidhar/Documents/purecode/solution_1/df_test.pkl'\n",
    "\n",
    "train_folder = '/home/vamsidhar/Documents/purecode/data_3/train/images'\n",
    "test_folder = '/home/vamsidhar/Documents/purecode/data_3/test/images'\n",
    "\n",
    "tokenizer_path = '/home/vamsidhar/Documents/purecode/solution_1/tokenizer.pkl'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# df_train = joblib.load(train_df_path)\n",
    "# df_test = joblib.load(test_df_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_weights(df):\n",
    "    cls = df.comp_label.unique()\n",
    "    \n",
    "    cls_count = df.comp_label.value_counts()\n",
    "    maximum = max(cls_count)\n",
    "    cls_count = maximum/cls_count\n",
    "    \n",
    "    weights = cls_count.to_dict()\n",
    "\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(max(df_train.comp_label.value_counts()))\n",
    "# a = 4636/df_train.comp_label.value_counts()\n",
    "# a.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gradient": {
     "editing": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Switch' 'Checkbox' 'Card' 'Icon' 'Button']\n",
      "{3: 1.0, 4: 1.758058399696625, 2: 2.136405529953917, 1: 3.6049766718506997, 0: 14.578616352201259}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes_train = df_train.comp_type.unique()\n",
    "classes_test  = df_test.comp_type.unique()\n",
    "print(classes_train)\n",
    "\n",
    "num_classes = len(classes_train)\n",
    "\n",
    "\n",
    "class_weights = get_weights(df=df_train)\n",
    "    \n",
    "print(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "\n",
    "def generate_filenames_label(df, shuffle=True, split=False, split_ratio=0.2):\n",
    "    if not split:\n",
    "        file_name_img = df.path_images.values\n",
    "        properties = list(df.text.values)\n",
    "#         label = label_encoding(df)\n",
    "        label = df.comp_label.values\n",
    "    \n",
    "        return label, properties, file_name_img\n",
    "    \n",
    "    else:\n",
    "        df_temp = df.copy()\n",
    "        \n",
    "        df_val = df_temp.sample(frac = 0.2, random_state=42)\n",
    "        df_train = df_temp.drop(df_val.index)\n",
    "\n",
    "        df_val.reset_index(inplace=True)\n",
    "        df_train.reset_index(inplace=True)\n",
    "\n",
    "        file_name_img_train = df_train.path_images.values\n",
    "        properties_train = list(df_train.text.values)\n",
    "#         label_train = label_encoding(df_train)\n",
    "        label_train = df_train.comp_label.values\n",
    "        \n",
    "        \n",
    "        file_name_img_val = df_val.path_images.values\n",
    "        properties_val = list(df_val.text.values)\n",
    "#         label_val = label_encoding(df_val)\n",
    "        label_val = df_val.comp_label.values\n",
    "        \n",
    "        \n",
    "        return label_train, properties_train, file_name_img_train, label_val, properties_val, file_name_img_val  \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_train, prop_train, file_name_train, label_val, prop_val, file_name_val = generate_filenames_label(df_train,\n",
    "                                                                                  shuffle=True,\n",
    "                                                                                  split=True,\n",
    "                                                                                  split_ratio=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_test, prop_test, file_name_test  = generate_filenames_label(df_test, shuffle=False, split=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer ,Seq2SeqTrainingArguments\n",
    "from transformers import TFVisionEncoderDecoderModel , ViTFeatureExtractor\n",
    "from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config : \n",
    "    ENCODER = \"google/vit-base-patch16-224\"\n",
    "    DECODER = \"gpt2\"\n",
    "    TRAIN_BATCH_SIZE = 24\n",
    "    VAL_BATCH_SIZE = 24\n",
    "    VAL_EPOCHS = 1\n",
    "    LR = 5e-5\n",
    "    SEED = 42\n",
    "    MAX_LEN = 128\n",
    "    SUMMARY_LEN = 20\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    MEAN = (0.485, 0.456, 0.406)\n",
    "    STD = (0.229, 0.224, 0.225)\n",
    "    TRAIN_PCT = 0.95\n",
    "    NUM_WORKERS = mp.cpu_count()\n",
    "    EPOCHS = 3\n",
    "    IMG_SIZE = (224,224)\n",
    "    LABEL_MASK = -100\n",
    "    TOP_K = 1000\n",
    "    TOP_P = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer ,  GPT2Config , default_data_collator\n",
    "\n",
    "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
    "    return outputs\n",
    "AutoTokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(config.ENCODER)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.DECODER)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "# tokenizer = joblib.load(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# img = cv2.imread(df_train.path_images[0])/255\n",
    "\n",
    "# feature_extractor([img, img, img], return_tensors='tf').pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, filenames, labels, props, tokenizer,\n",
    "                 feature_extractor, shuffle=True):\n",
    "        \n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "        self.props = props\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.df = df\n",
    "#         self.transform = transform\n",
    "#         self.root_dir = root_dir\n",
    "        self.max_length = 128\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.props[idx]\n",
    "        img_path = self.filenames[idx]\n",
    "#         img = self.__load_image__(img_path)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             img= self.transform(img)\n",
    "        pixel_values = self.feature_extractor(img, return_tensors=\"pt\").pixel_values\n",
    "        captions = self.tokenizer(caption,\n",
    "                                 padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                 max_length=self.max_length).input_ids\n",
    "        \n",
    "        captions = [caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(captions)}\n",
    "        return encoding\n",
    "    \n",
    "    \n",
    "#     def __load_image__(self, image_path):\n",
    "#         img = cv2.imread(image_path)\n",
    "#         img = self.__resize_image__(img, pad=True)\n",
    "            \n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        \n",
    "#         return img\n",
    "\n",
    "#     def __resize_image__(self, img, pad=True):\n",
    "#         old_size = img.shape[:2]\n",
    "#         ratio = float(self.dim[0])/max(old_size)\n",
    "#         new_size = tuple([int(x*ratio) for x in old_size])\n",
    "#         img_new = cv2.resize(img, (new_size[1], new_size[0])) \n",
    "\n",
    "#         #resize by padding\n",
    "#         delta_w = self.dim[1] - new_size[1]\n",
    "#         delta_h = self.dim[0] - new_size[0]\n",
    "#         top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "#         left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "#         color = [0, 0, 0]\n",
    "#         img_new = cv2.copyMakeBorder(img_new, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "#             value=color)\n",
    "\n",
    "        \n",
    "#         return img_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImgDataset(filenames=file_name_train, labels=label_train, props=prop_train,\n",
    "                           tokenizer=tokenizer,feature_extractor = feature_extractor )\n",
    "\n",
    "val_dataset = ImgDataset(filenames=file_name_val, labels=label_val, props=prop_val,\n",
    "                           tokenizer=tokenizer,feature_extractor = feature_extractor )\n",
    "\n",
    "test_dataset = ImgDataset(filenames=file_name_test, labels=label_val, props=prop_test,\n",
    "                           tokenizer=tokenizer,feature_extractor = feature_extractor )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.crossattention.c_attn.weight', 'h.2.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.1.crossattention.bias', 'h.0.crossattention.c_proj.bias', 'h.6.crossattention.bias', 'h.11.crossattention.masked_bias', 'h.7.crossattention.c_proj.bias', 'h.1.crossattention.masked_bias', 'h.2.crossattention.bias', 'h.11.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.0.crossattention.bias', 'h.9.crossattention.c_proj.weight', 'h.7.crossattention.masked_bias', 'h.9.ln_cross_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.9.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.3.crossattention.bias', 'h.0.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.weight', 'h.5.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.masked_bias', 'h.10.crossattention.c_proj.weight', 'h.5.crossattention.bias', 'h.10.crossattention.bias', 'h.9.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.2.crossattention.c_attn.weight', 'h.2.ln_cross_attn.weight', 'h.5.crossattention.masked_bias', 'h.6.crossattention.c_attn.weight', 'h.8.crossattention.masked_bias', 'h.9.crossattention.masked_bias', 'h.4.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.bias', 'h.4.crossattention.bias', 'h.5.crossattention.q_attn.weight', 'h.7.crossattention.bias', 'h.11.crossattention.bias', 'h.7.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.8.ln_cross_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.0.crossattention.masked_bias', 'h.4.ln_cross_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.6.crossattention.masked_bias', 'h.1.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.4.crossattention.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(config.ENCODER, \n",
    "                                                                  config.DECODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.max_length = 128\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='VIT_large_gpt2',\n",
    "    per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.VAL_BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1024,  \n",
    "    save_steps=2048, \n",
    "    warmup_steps=1024,  \n",
    "    learning_rate = 5e-5,\n",
    "    #max_steps=1500, # delete for full training\n",
    "    num_train_epochs = config.EPOCHS, #TRAIN_EPOCHS\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "import cv2\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    tokenizer=feature_extractor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8838\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1107\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvamsidhar_muthireddy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec23b4c25dc438aaba6951f29f3fada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670543799409642, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20230402_175609-2vxpkgay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vamsidhar_muthireddy/huggingface/runs/2vxpkgay\" target=\"_blank\">VIT_large_gpt2</a></strong> to <a href=\"https://wandb.ai/vamsidhar_muthireddy/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1107' max='1107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1107/1107 2:26:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.447180</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.332300</td>\n",
       "      <td>0.165400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.305749</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.340100</td>\n",
       "      <td>0.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.710500</td>\n",
       "      <td>0.223165</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.375500</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2209\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2209\n",
      "  Batch size = 24\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2209\n",
      "  Batch size = 24\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1107, training_loss=0.6768736163021318, metrics={'train_runtime': 8817.2323, 'train_samples_per_second': 3.007, 'train_steps_per_second': 0.126, 'total_flos': 4.784819609159074e+18, 'train_loss': 0.6768736163021318, 'epoch': 3.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to VIT_large_gpt2\n",
      "Configuration saved in VIT_large_gpt2/config.json\n",
      "Model weights saved in VIT_large_gpt2/pytorch_model.bin\n",
      "Feature extractor saved in VIT_large_gpt2/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('VIT_large_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_extractor.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(feature_extractor, \"feature_extractor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump( model, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "test_image_path = df_test.path_images[test_index]\n",
    "img =  Image.open(test_image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"componentType\": \"Button\", \"componentProps/sx/color\": \"#ffffff\", \"completed\": false, \"committed\": true, \"componentStatus\": \"inherit\", \"sx/_textTransform\": \"none\", \"my\": \"0.3rem\", \"m\": 1, \"mySpacing\": \"1.5px\", \"mx\": 2, \"border\": \"2px solid #b5b568b5\", \"borderRadius\": \"10px\", \"-webkit-fontSize\": \"13px\", -webkit-textTransform: \"none\"}, \"\n"
     ]
    }
   ],
   "source": [
    "# generated_caption = tokenizer.decode(model.generate(feature_extractor(img, return_tensors=\"pt\").pixel_values.to(\"cuda\"))[0])\n",
    "generated_caption = tokenizer.decode(model.generate(feature_extractor(img, return_tensors=\"pt\").pixel_values.to(\"cuda\"))[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"componentType\": \"Button\", \"componentProps/variant\": \"outlined\", \"componentProps/sx/color\": \"grey\", \"componentProps/sx/bgcolor\": \"white\", \"componentProps/sx/border\": \"1px solid grey\", \"componentProps/sx/textTransform\": \"none\", \"componentProps/sx/p\": \"0.25rem 5rem 0.25rem 0.35rem\", \"componentProps/sx/fontSize\": \"18px\", \"componentProps/sx/fontWeight\": 500, \"componentProps/sx/width\": \"189.078125px\", \"componentProps/sx/height\": \"41.5px\", \"componentProps/size\": \"large\", \"componentProps/disableElevation\": true}'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.text[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"componentType\": \"Button\", \"componentProps/sx/color\": \"#ffffff\", \"completed\": false, \"committed\": true, \"componentStatus\": \"inherit\", \"sx/_textTransform\": \"none\", \"my\": \"0.3rem\", \"m\": 1, \"mySpacing\": \"1.5px\", \"mx\": 2, \"border\": \"2px solid #b5b568b5\", \"borderRadius\": \"10px\", \"-webkit-fontSize\": \"13px\", -webkit-textTransform: \"none\"}, \"\n"
     ]
    }
   ],
   "source": [
    "generated_caption = tokenizer_loaded.decode(model_loaded.generate(feature_extractor_loaded(img, return_tensors=\"pt\").pixel_values.to(\"cuda\"))[0], \n",
    "                                            skip_special_tokens=True)\n",
    "\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f105423b4adf4cc634b98cf4481e00717d8221271c0e5b06e1f86191b8e1036b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
